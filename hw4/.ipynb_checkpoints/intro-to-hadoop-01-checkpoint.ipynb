{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Introduction to Hadoop MapReduce </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python Jupyter notebook supports execution of Linux command inside the notebook cells. This is done by adding the **!** to the beginning of the command line. It should be noted that each command begins with a **!** will create a new bash shell and close this cell once the execution is done:\n",
    "- Full path is required\n",
    "- Temporary results and environmental variables will be lost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to initialize Kerberos authentication mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interaction with Hadoop Distributed File System is done through `hdfs` and its sub-commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namenode hostname node0367.palmetto.clemson.edu\n",
      "Check temp dir creations\n",
      "node0279.palmetto.clemson.edu\n",
      "\n",
      "The following have been reloaded with a version change:\n",
      "  1) libxml2/2.9.10-gcc/5.4.0 => libxml2/2.9.10-gcc/8.3.1\n",
      "\n",
      "node0279.palmetto.clemson.edu\n",
      "total 0\n",
      "-rw-r--r-- 1 jin6 cuuser 0 Nov  4 11:24 test\n",
      "Connection to node0279.palmetto.clemson.edu closed.\n",
      "node0280.palmetto.clemson.edu\n",
      "\n",
      "The following have been reloaded with a version change:\n",
      "  1) libxml2/2.9.10-gcc/5.4.0 => libxml2/2.9.10-gcc/8.3.1\n",
      "\n",
      "node0280.palmetto.clemson.edu\n",
      "total 0\n",
      "drwxr-xr-x 4 jin6 cuuser 28 Nov  4 11:19 hadoop\n",
      "drwxr-xr-x 3 jin6 cuuser 22 Nov  4 11:19 hdfs\n",
      "-rw-r--r-- 1 jin6 cuuser  0 Nov  4 11:24 test\n",
      "drwxr-xr-x 4 jin6 cuuser 30 Nov  4 11:20 yarn\n",
      "Connection to node0280.palmetto.clemson.edu closed.\n",
      "node0282.palmetto.clemson.edu\n",
      "\n",
      "The following have been reloaded with a version change:\n",
      "  1) libxml2/2.9.10-gcc/5.4.0 => libxml2/2.9.10-gcc/8.3.1\n",
      "\n",
      "node0282.palmetto.clemson.edu\n",
      "total 0\n",
      "drwxr-xr-x 4 jin6 cuuser 28 Nov  4 11:20 hadoop\n",
      "drwxr-xr-x 3 jin6 cuuser 22 Nov  4 11:20 hdfs\n",
      "-rw-r--r-- 1 jin6 cuuser  0 Nov  4 11:24 test\n",
      "drwxr-xr-x 4 jin6 cuuser 30 Nov  4 11:20 yarn\n",
      "Connection to node0282.palmetto.clemson.edu closed.\n",
      "node0367.palmetto.clemson.edu\n",
      "\n",
      "The following have been reloaded with a version change:\n",
      "  1) libxml2/2.9.10-gcc/5.4.0 => libxml2/2.9.10-gcc/8.3.1\n",
      "\n",
      "node0367.palmetto.clemson.edu\n",
      "total 0\n",
      "drwxr-xr-x 4 jin6 cuuser 28 Nov  4 11:20 hadoop\n",
      "drwxr-xr-x 3 jin6 cuuser 22 Nov  4 11:20 hdfs\n",
      "drwxr-xr-x 3 jin6 cuuser 17 Nov  4 11:15 npm-2554516-952065a3\n",
      "-rw-r--r-- 1 jin6 cuuser  0 Nov  4 11:24 test\n",
      "drwxr-xr-x 4 jin6 cuuser 30 Nov  4 11:20 yarn\n",
      "Connection to node0367.palmetto.clemson.edu closed.\n",
      "HADOOP_ROOT: /software/spackages/linux-centos8-x86_64/gcc-8.3.1/hadoop-3.2.1-lux74iyfcxxzatxh4em67xezesi4z4i4\n",
      "JAVA_HOME: /software/spackages/linux-centos8-x86_64/gcc-8.3.1/openjdk-1.8.0_222-b10-ui6ldtk7sly7izptgo3yptahiqabz4fd\n",
      "NameNode is: node0367.palmetto.clemson.edu\n",
      "Resource Manager Node is: node0367.palmetto.clemson.edu\n",
      "NameNode\n",
      "    Identifier: node0367.palmetto.clemson.edu\n",
      "    Services: NameNode, JobHistoryServer, HMaster\n",
      "\n",
      "Resource Manager Node\n",
      "    Identifier: node0367.palmetto.clemson.edu\n",
      "    Services: Resource Manager, Secondary NameNode\n",
      "\n",
      "\n",
      "The following have been reloaded with a version change:\n",
      "  1) libxml2/2.9.10-gcc/5.4.0 => libxml2/2.9.10-gcc/8.3.1\n",
      "\n",
      "Connection to node0280.palmetto.clemson.edu closed.\n",
      "\n",
      "The following have been reloaded with a version change:\n",
      "  1) libxml2/2.9.10-gcc/5.4.0 => libxml2/2.9.10-gcc/8.3.1\n",
      "\n",
      "Connection to node0280.palmetto.clemson.edu closed.\n",
      "\n",
      "The following have been reloaded with a version change:\n",
      "  1) libxml2/2.9.10-gcc/5.4.0 => libxml2/2.9.10-gcc/8.3.1\n",
      "\n",
      "Connection to node0280.palmetto.clemson.edu closed.\n",
      "\n",
      "The following have been reloaded with a version change:\n",
      "  1) libxml2/2.9.10-gcc/5.4.0 => libxml2/2.9.10-gcc/8.3.1\n",
      "\n",
      "Connection to node0280.palmetto.clemson.edu closed.\n",
      "\n",
      "The following have been reloaded with a version change:\n",
      "  1) libxml2/2.9.10-gcc/5.4.0 => libxml2/2.9.10-gcc/8.3.1\n",
      "\n",
      "Connection to node0280.palmetto.clemson.edu closed.\n",
      "\n",
      "The following have been reloaded with a version change:\n",
      "  1) libxml2/2.9.10-gcc/5.4.0 => libxml2/2.9.10-gcc/8.3.1\n",
      "\n",
      "Connection to node0282.palmetto.clemson.edu closed.\n",
      "\n",
      "The following have been reloaded with a version change:\n",
      "  1) libxml2/2.9.10-gcc/5.4.0 => libxml2/2.9.10-gcc/8.3.1\n",
      "\n",
      "Connection to node0282.palmetto.clemson.edu closed.\n",
      "\n",
      "The following have been reloaded with a version change:\n",
      "  1) libxml2/2.9.10-gcc/5.4.0 => libxml2/2.9.10-gcc/8.3.1\n",
      "\n",
      "Connection to node0282.palmetto.clemson.edu closed.\n",
      "\n",
      "The following have been reloaded with a version change:\n",
      "  1) libxml2/2.9.10-gcc/5.4.0 => libxml2/2.9.10-gcc/8.3.1\n",
      "\n",
      "Connection to node0282.palmetto.clemson.edu closed.\n",
      "\n",
      "The following have been reloaded with a version change:\n",
      "  1) libxml2/2.9.10-gcc/5.4.0 => libxml2/2.9.10-gcc/8.3.1\n",
      "\n",
      "Connection to node0282.palmetto.clemson.edu closed.\n",
      "\n",
      "The following have been reloaded with a version change:\n",
      "  1) libxml2/2.9.10-gcc/5.4.0 => libxml2/2.9.10-gcc/8.3.1\n",
      "\n",
      "Connection to node0367.palmetto.clemson.edu closed.\n",
      "\n",
      "The following have been reloaded with a version change:\n",
      "  1) libxml2/2.9.10-gcc/5.4.0 => libxml2/2.9.10-gcc/8.3.1\n",
      "\n",
      "Connection to node0367.palmetto.clemson.edu closed.\n",
      "\n",
      "The following have been reloaded with a version change:\n",
      "  1) libxml2/2.9.10-gcc/5.4.0 => libxml2/2.9.10-gcc/8.3.1\n",
      "\n",
      "Connection to node0367.palmetto.clemson.edu closed.\n",
      "\n",
      "The following have been reloaded with a version change:\n",
      "  1) libxml2/2.9.10-gcc/5.4.0 => libxml2/2.9.10-gcc/8.3.1\n",
      "\n",
      "Connection to node0367.palmetto.clemson.edu closed.\n",
      "\n",
      "The following have been reloaded with a version change:\n",
      "  1) libxml2/2.9.10-gcc/5.4.0 => libxml2/2.9.10-gcc/8.3.1\n",
      "\n",
      "Connection to node0367.palmetto.clemson.edu closed.\n",
      "Begin launching Hadoop cluster ...\n",
      "WARNING: /local_scratch/pbs.828491.pbs02/hadoop/log does not exist. Creating.\n",
      "mkdir: cannot create directory ‘/local_scratch/pbs.828491.pbs02’: Permission denied\n",
      "ERROR: Unable to create /local_scratch/pbs.828491.pbs02/hadoop/log. Aborting.\n",
      "\n",
      "real\t0m1.000s\n",
      "user\t0m0.826s\n",
      "sys\t0m0.093s\n",
      "+ /software/spackages/linux-centos8-x86_64/gcc-8.3.1/hadoop-3.2.1-lux74iyfcxxzatxh4em67xezesi4z4i4/sbin/start-dfs.sh --config /home/jin6/hadoop_palmetto/config\n",
      "Starting namenodes on [node0297.palmetto.clemson.edu]\n",
      "node0297.palmetto.clemson.edu: \n",
      "node0297.palmetto.clemson.edu: The following have been reloaded with a version change:\n",
      "node0297.palmetto.clemson.edu:   1) libxml2/2.9.10-gcc/5.4.0 => libxml2/2.9.10-gcc/8.3.1\n",
      "node0297.palmetto.clemson.edu: \n",
      "Starting datanodes\n",
      "node0317.palmetto.clemson.edu: \n",
      "node0317.palmetto.clemson.edu: The following have been reloaded with a version change:\n",
      "node0317.palmetto.clemson.edu:   1) libxml2/2.9.10-gcc/5.4.0 => libxml2/2.9.10-gcc/8.3.1\n",
      "node0317.palmetto.clemson.edu: \n",
      "node0361.palmetto.clemson.edu: \n",
      "node0361.palmetto.clemson.edu: The following have been reloaded with a version change:\n",
      "node0361.palmetto.clemson.edu:   1) libxml2/2.9.10-gcc/5.4.0 => libxml2/2.9.10-gcc/8.3.1\n",
      "node0361.palmetto.clemson.edu: \n",
      "node0361.palmetto.clemson.edu: WARNING: /local_scratch/pbs.828491.pbs02/hadoop/log does not exist. Creating.\n",
      "Starting secondary namenodes [node0367.palmetto.clemson.edu]\n",
      "node0367.palmetto.clemson.edu: \n",
      "node0367.palmetto.clemson.edu: The following have been reloaded with a version change:\n",
      "node0367.palmetto.clemson.edu:   1) libxml2/2.9.10-gcc/5.4.0 => libxml2/2.9.10-gcc/8.3.1\n",
      "node0367.palmetto.clemson.edu: \n",
      "node0367.palmetto.clemson.edu: WARNING: /local_scratch/pbs.828491.pbs02/hadoop/log does not exist. Creating.\n",
      "node0367.palmetto.clemson.edu: mkdir: cannot create directory ‘/local_scratch/pbs.828491.pbs02’: Permission denied\n",
      "node0367.palmetto.clemson.edu: ERROR: Unable to create /local_scratch/pbs.828491.pbs02/hadoop/log. Aborting.\n",
      "2020-11-04 11:25:33,797 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\n",
      "real\t0m23.226s\n",
      "user\t0m12.802s\n",
      "sys\t0m1.392s\n",
      "+ /software/spackages/linux-centos8-x86_64/gcc-8.3.1/hadoop-3.2.1-lux74iyfcxxzatxh4em67xezesi4z4i4/sbin/start-yarn.sh --config /home/jin6/hadoop_palmetto/config\n",
      "WARNING: YARN_CONF_DIR has been replaced by HADOOP_CONF_DIR. Using value of YARN_CONF_DIR.\n",
      "WARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\n",
      "Starting resourcemanager\n",
      "WARNING: YARN_CONF_DIR has been replaced by HADOOP_CONF_DIR. Using value of YARN_CONF_DIR.\n",
      "WARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\n",
      "Starting nodemanagers\n",
      "WARNING: YARN_CONF_DIR has been replaced by HADOOP_CONF_DIR. Using value of YARN_CONF_DIR.\n",
      "WARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\n",
      "node0317.palmetto.clemson.edu: \n",
      "node0317.palmetto.clemson.edu: The following have been reloaded with a version change:\n",
      "node0317.palmetto.clemson.edu:   1) libxml2/2.9.10-gcc/5.4.0 => libxml2/2.9.10-gcc/8.3.1\n",
      "node0317.palmetto.clemson.edu: \n",
      "node0361.palmetto.clemson.edu: \n",
      "node0361.palmetto.clemson.edu: The following have been reloaded with a version change:\n",
      "node0361.palmetto.clemson.edu:   1) libxml2/2.9.10-gcc/5.4.0 => libxml2/2.9.10-gcc/8.3.1\n",
      "node0361.palmetto.clemson.edu: \n",
      "\n",
      "real\t0m12.888s\n",
      "user\t0m6.877s\n",
      "sys\t0m0.903s\n",
      "+ echo 'Waiting for datanodes to launch ...'\n",
      "Waiting for datanodes to launch ...\n",
      "+ sleep 10\n",
      "+ /software/spackages/linux-centos8-x86_64/gcc-8.3.1/hadoop-3.2.1-lux74iyfcxxzatxh4em67xezesi4z4i4/bin/hdfs --config /home/jin6/hadoop_palmetto/config dfsadmin -report\n",
      "2020-11-04 11:25:58,878 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Configured Capacity: 365420093440 (340.32 GB)\n",
      "Present Capacity: 350881562624 (326.78 GB)\n",
      "DFS Remaining: 350881554432 (326.78 GB)\n",
      "DFS Used: 8192 (8 KB)\n",
      "DFS Used%: 0.00%\n",
      "Replicated Blocks:\n",
      "\tUnder replicated blocks: 0\n",
      "\tBlocks with corrupt replicas: 0\n",
      "\tMissing blocks: 0\n",
      "\tMissing blocks (with replication factor 1): 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 0\n",
      "Erasure Coded Block Groups: \n",
      "\tLow redundancy block groups: 0\n",
      "\tBlock groups with corrupt internal blocks: 0\n",
      "\tMissing block groups: 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 0\n",
      "\n",
      "-------------------------------------------------\n",
      "Live datanodes (2):\n",
      "\n",
      "Name: 10.125.2.102:9866 (node0361.palmetto.clemson.edu)\n",
      "Hostname: node0361.palmetto.clemson.edu\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 182710046720 (170.16 GB)\n",
      "DFS Used: 4096 (4 KB)\n",
      "Non DFS Used: 7030861824 (6.55 GB)\n",
      "DFS Remaining: 175679180800 (163.61 GB)\n",
      "DFS Used%: 0.00%\n",
      "DFS Remaining%: 96.15%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 1\n",
      "Last contact: Wed Nov 04 11:25:58 EST 2020\n",
      "Last Block Report: Wed Nov 04 11:25:52 EST 2020\n",
      "Num of Blocks: 0\n",
      "\n",
      "\n",
      "Name: 10.125.2.58:9866 (node0317.palmetto.clemson.edu)\n",
      "Hostname: node0317.palmetto.clemson.edu\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 182710046720 (170.16 GB)\n",
      "DFS Used: 4096 (4 KB)\n",
      "Non DFS Used: 7507668992 (6.99 GB)\n",
      "DFS Remaining: 175202373632 (163.17 GB)\n",
      "DFS Used%: 0.00%\n",
      "DFS Remaining%: 95.89%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 1\n",
      "Last contact: Wed Nov 04 11:25:57 EST 2020\n",
      "Last Block Report: Wed Nov 04 11:25:57 EST 2020\n",
      "Num of Blocks: 0\n",
      "\n",
      "\n",
      "\n",
      "real\t0m3.284s\n",
      "user\t0m4.707s\n",
      "sys\t0m0.414s\n"
     ]
    }
   ],
   "source": [
    "!./init_hadoop.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: hdfs [OPTIONS] SUBCOMMAND [SUBCOMMAND OPTIONS]\n",
      "\n",
      "  OPTIONS is none or any of:\n",
      "\n",
      "--buildpaths                       attempt to add class files from build tree\n",
      "--config dir                       Hadoop config directory\n",
      "--daemon (start|status|stop)       operate on a daemon\n",
      "--debug                            turn on shell script debug mode\n",
      "--help                             usage information\n",
      "--hostnames list[,of,host,names]   hosts to use in worker mode\n",
      "--hosts filename                   list of hosts to use in worker mode\n",
      "--loglevel level                   set the log4j level for this command\n",
      "--workers                          turn on worker mode\n",
      "\n",
      "  SUBCOMMAND is one of:\n",
      "\n",
      "\n",
      "    Admin Commands:\n",
      "\n",
      "cacheadmin           configure the HDFS cache\n",
      "crypto               configure HDFS encryption zones\n",
      "debug                run a Debug Admin to execute HDFS debug commands\n",
      "dfsadmin             run a DFS admin client\n",
      "dfsrouteradmin       manage Router-based federation\n",
      "ec                   run a HDFS ErasureCoding CLI\n",
      "fsck                 run a DFS filesystem checking utility\n",
      "haadmin              run a DFS HA admin client\n",
      "jmxget               get JMX exported values from NameNode or DataNode.\n",
      "oev                  apply the offline edits viewer to an edits file\n",
      "oiv                  apply the offline fsimage viewer to an fsimage\n",
      "oiv_legacy           apply the offline fsimage viewer to a legacy fsimage\n",
      "storagepolicies      list/get/set/satisfyStoragePolicy block storage policies\n",
      "\n",
      "    Client Commands:\n",
      "\n",
      "classpath            prints the class path needed to get the hadoop jar and\n",
      "                     the required libraries\n",
      "dfs                  run a filesystem command on the file system\n",
      "envvars              display computed Hadoop environment variables\n",
      "fetchdt              fetch a delegation token from the NameNode\n",
      "getconf              get config values from configuration\n",
      "groups               get the groups which users belong to\n",
      "lsSnapshottableDir   list all snapshottable dirs owned by the current user\n",
      "snapshotDiff         diff two snapshots of a directory or diff the current\n",
      "                     directory contents with a snapshot\n",
      "version              print the version\n",
      "\n",
      "    Daemon Commands:\n",
      "\n",
      "balancer             run a cluster balancing utility\n",
      "datanode             run a DFS datanode\n",
      "dfsrouter            run the DFS router\n",
      "diskbalancer         Distributes data evenly among disks on a given node\n",
      "httpfs               run HttpFS server, the HDFS HTTP Gateway\n",
      "journalnode          run the DFS journalnode\n",
      "mover                run a utility to move block replicas across storage types\n",
      "namenode             run the DFS namenode\n",
      "nfs3                 run an NFS version 3 gateway\n",
      "portmap              run a portmap service\n",
      "secondarynamenode    run the DFS secondary namenode\n",
      "sps                  run external storagepolicysatisfier\n",
      "zkfc                 run the ZK Failover Controller daemon\n",
      "\n",
      "SUBCOMMAND may print help when invoked w/o parameters or with -h.\n"
     ]
    }
   ],
   "source": [
    "!hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-04 11:26:17,315 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Usage: hadoop fs [generic options]\n",
      "\t[-appendToFile <localsrc> ... <dst>]\n",
      "\t[-cat [-ignoreCrc] <src> ...]\n",
      "\t[-checksum <src> ...]\n",
      "\t[-chgrp [-R] GROUP PATH...]\n",
      "\t[-chmod [-R] <MODE[,MODE]... | OCTALMODE> PATH...]\n",
      "\t[-chown [-R] [OWNER][:[GROUP]] PATH...]\n",
      "\t[-copyFromLocal [-f] [-p] [-l] [-d] [-t <thread count>] <localsrc> ... <dst>]\n",
      "\t[-copyToLocal [-f] [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]\n",
      "\t[-count [-q] [-h] [-v] [-t [<storage type>]] [-u] [-x] [-e] <path> ...]\n",
      "\t[-cp [-f] [-p | -p[topax]] [-d] <src> ... <dst>]\n",
      "\t[-createSnapshot <snapshotDir> [<snapshotName>]]\n",
      "\t[-deleteSnapshot <snapshotDir> <snapshotName>]\n",
      "\t[-df [-h] [<path> ...]]\n",
      "\t[-du [-s] [-h] [-v] [-x] <path> ...]\n",
      "\t[-expunge [-immediate]]\n",
      "\t[-find <path> ... <expression> ...]\n",
      "\t[-get [-f] [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]\n",
      "\t[-getfacl [-R] <path>]\n",
      "\t[-getfattr [-R] {-n name | -d} [-e en] <path>]\n",
      "\t[-getmerge [-nl] [-skip-empty-file] <src> <localdst>]\n",
      "\t[-head <file>]\n",
      "\t[-help [cmd ...]]\n",
      "\t[-ls [-C] [-d] [-h] [-q] [-R] [-t] [-S] [-r] [-u] [-e] [<path> ...]]\n",
      "\t[-mkdir [-p] <path> ...]\n",
      "\t[-moveFromLocal <localsrc> ... <dst>]\n",
      "\t[-moveToLocal <src> <localdst>]\n",
      "\t[-mv <src> ... <dst>]\n",
      "\t[-put [-f] [-p] [-l] [-d] <localsrc> ... <dst>]\n",
      "\t[-renameSnapshot <snapshotDir> <oldName> <newName>]\n",
      "\t[-rm [-f] [-r|-R] [-skipTrash] [-safely] <src> ...]\n",
      "\t[-rmdir [--ignore-fail-on-non-empty] <dir> ...]\n",
      "\t[-setfacl [-R] [{-b|-k} {-m|-x <acl_spec>} <path>]|[--set <acl_spec> <path>]]\n",
      "\t[-setfattr {-n name [-v value] | -x name} <path>]\n",
      "\t[-setrep [-R] [-w] <rep> <path> ...]\n",
      "\t[-stat [format] <path> ...]\n",
      "\t[-tail [-f] [-s <sleep interval>] <file>]\n",
      "\t[-test -[defswrz] <path>]\n",
      "\t[-text [-ignoreCrc] <src> ...]\n",
      "\t[-touch [-a] [-m] [-t TIMESTAMP ] [-c] <path> ...]\n",
      "\t[-touchz <path> ...]\n",
      "\t[-truncate [-w] <length> <path> ...]\n",
      "\t[-usage [cmd ...]]\n",
      "\n",
      "Generic options supported are:\n",
      "-conf <configuration file>        specify an application configuration file\n",
      "-D <property=value>               define a value for a given property\n",
      "-fs <file:///|hdfs://namenode:port> specify default filesystem URL to use, overrides 'fs.defaultFS' property from configurations.\n",
      "-jt <local|resourcemanager:port>  specify a ResourceManager\n",
      "-files <file1,...>                specify a comma-separated list of files to be copied to the map reduce cluster\n",
      "-libjars <jar1,...>               specify a comma-separated list of jar files to be included in the classpath\n",
      "-archives <archive1,...>          specify a comma-separated list of archives to be unarchived on the compute machines\n",
      "\n",
      "The general command line syntax is:\n",
      "command [genericOptions] [commandOptions]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We need to specify the location of the configuration files for our hadoop cluster EVERYTIME we run, otherwise the command will read from the default config location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-04 11:26:21,539 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 38 items\n",
      "-rw-r--r--   1 root root          0 2020-07-25 14:47 /.autorelabel\n",
      "-rw-r--r--   1 root root          0 2020-07-25 14:40 /1\n",
      "dr-xr-xr-x   - root root      69632 2020-10-06 11:01 /bin\n",
      "dr-xr-xr-x   - root root       4096 2020-07-25 14:47 /boot\n",
      "drwxr-xr-x   - root root          6 2020-07-25 14:39 /burstbuffer\n",
      "drwxr-xr-x   - root root          6 2020-07-25 14:40 /curium\n",
      "drwxr-xr-x   - root root          0 2020-11-04 11:06 /cvmfs\n",
      "drwxr-xr-x   - root root       3560 2020-10-26 14:50 /dev\n",
      "drwxr-xr-x   - root root      12288 2020-11-04 08:54 /etc\n",
      "drwxr-xr-x   - root root          6 2020-07-25 14:40 /feltus\n",
      "drwxr-xr-x   - root root       1955 2020-11-04 08:49 /home\n",
      "dr-xr-xr-x   - root root       8192 2020-09-19 06:52 /lib\n",
      "dr-xr-xr-x   - root root      81920 2020-10-30 12:24 /lib64\n",
      "drwxr-xr-x   - root root        162 2020-11-04 11:14 /local_scratch\n",
      "drwxr-xr-x   - root root          6 2019-05-10 20:33 /media\n",
      "drwxr-xr-x   - root root          0 2020-08-19 18:11 /misc\n",
      "drwxr-xr-x   - root root          6 2019-05-10 20:33 /mnt\n",
      "drwxr-xr-x   - root root          0 2020-08-19 18:11 /net\n",
      "drwxr-xr-x   - root root        124 2020-07-25 14:40 /opt\n",
      "drwxr-xr-x   - root root         23 2020-07-25 14:47 /osg\n",
      "drwxr-xr-x   - root root          6 2020-07-25 14:40 /panicle\n",
      "drwxr-xr-x   - root root       1976 2020-11-04 08:49 /parallel-scratch1\n",
      "drwxr-xr-x   - root root       4096 2020-10-02 15:33 /perfdata\n",
      "dr-xr-xr-x   - root root          0 2020-08-07 14:20 /proc\n",
      "drwxr-xr-x   - root root         22 2020-07-25 14:39 /pscratch\n",
      "dr-xr-x---   - root root        218 2020-10-16 10:08 /root\n",
      "drwxr-xr-x   - root root       1440 2020-10-22 11:32 /run\n",
      "dr-xr-xr-x   - root root      20480 2020-09-19 06:52 /sbin\n",
      "drwxr-xr-x   - root root       1976 2020-11-04 08:49 /scratch1\n",
      "drwxr-xr-x   - root root       1969 2020-11-04 08:49 /scratch2\n",
      "drwxr-xr-x   - root root       4096 2020-10-23 13:51 /software\n",
      "drwxr-xr-x   - root root          6 2019-05-10 20:33 /srv\n",
      "dr-xr-xr-x   - root root          0 2020-08-07 14:20 /sys\n",
      "drwxrwxrwt   - root root       4096 2020-11-04 11:25 /tmp\n",
      "drwxr-xr-x   - root root        144 2020-07-25 14:28 /usr\n",
      "drwxr-xr-x   - root root       4096 2020-07-25 14:47 /var\n",
      "drwx------   - root root       8192 2020-07-25 14:39 /xcatpost\n",
      "drwxr-xr-x   - root root       4096 2020-10-29 17:41 /zfs\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Currently Loaded Modules:\n",
      "  1) anaconda3/5.1.0-gcc/8.3.1   4) openmpi/1.10.3-gcc/5.4.0-cuda9_2\n",
      "  2) libxml2/2.9.10-gcc/5.4.0    5) openjdk/1.8.0_222-b10-gcc/8.3.1\n",
      "  3) cuda/9.2.88-gcc/5.4.0       6) hadoop/3.2.1-gcc/8.3.1\n",
      "\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "!module list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-04 11:28:15,435 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "!hdfs --config ~/hadoop_palmetto/config dfs -ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-04 11:30:24,684 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Connecting to namenode via http://node0297.palmetto.clemson.edu:9870/fsck?ugi=jin6&path=%2F\n",
      "FSCK started by jin6 (auth:SIMPLE) from /10.125.2.108 for path / at Wed Nov 04 11:30:25 EST 2020\n",
      "\n",
      "Status: HEALTHY\n",
      " Number of data-nodes:\t2\n",
      " Number of racks:\t\t1\n",
      " Total dirs:\t\t\t1\n",
      " Total symlinks:\t\t0\n",
      "\n",
      "Replicated Blocks:\n",
      " Total size:\t0 B\n",
      " Total files:\t0\n",
      " Total blocks (validated):\t0\n",
      " Minimally replicated blocks:\t0\n",
      " Over-replicated blocks:\t0\n",
      " Under-replicated blocks:\t0\n",
      " Mis-replicated blocks:\t\t0\n",
      " Default replication factor:\t3\n",
      " Average block replication:\t0.0\n",
      " Missing blocks:\t\t0\n",
      " Corrupt blocks:\t\t0\n",
      " Missing replicas:\t\t0\n",
      "\n",
      "Erasure Coded Block Groups:\n",
      " Total size:\t0 B\n",
      " Total files:\t0\n",
      " Total block groups (validated):\t0\n",
      " Minimally erasure-coded block groups:\t0\n",
      " Over-erasure-coded block groups:\t0\n",
      " Under-erasure-coded block groups:\t0\n",
      " Unsatisfactory placement block groups:\t0\n",
      " Average block group size:\t0.0\n",
      " Missing block groups:\t\t0\n",
      " Corrupt block groups:\t\t0\n",
      " Missing internal blocks:\t0\n",
      "FSCK ended at Wed Nov 04 11:30:25 EST 2020 in 1 milliseconds\n",
      "\n",
      "\n",
      "The filesystem under path '/' is HEALTHY\n"
     ]
    }
   ],
   "source": [
    "!hdfs --config ~/hadoop_palmetto/config fsck /"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "Create a directory named **intro-to-hadoop** inside your user directory on HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-04 11:35:20,117 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "mkdir: `/user': File exists\n"
     ]
    }
   ],
   "source": [
    "!hdfs --config ~/hadoop_palmetto/config dfs -mkdir /user/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-04 11:35:40,987 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "mkdir: `/user/jin6': File exists\n"
     ]
    }
   ],
   "source": [
    "!hdfs --config ~/hadoop_palmetto/config dfs -mkdir /user/jin6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-04 11:47:02,254 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 1 items\n",
      "drwxr-xr-x   - jin6 supergroup          0 2020-11-04 11:39 /user/jin6/intro-to-hadoop\n"
     ]
    }
   ],
   "source": [
    "!hdfs --config ~/hadoop_palmetto/config dfs -ls /user/jin6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-04 11:37:21,953 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "mkdir: `/user/jin6/intro-to-hadoop': File exists\n"
     ]
    }
   ],
   "source": [
    "!hdfs --config ~/hadoop_palmetto/config dfs -mkdir /user/jin6/intro-to-hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-04 11:37:28,384 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "!hdfs --config ~/hadoop_palmetto/config dfs -ls /user/jin6/intro-to-hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "Upload the **text** directory into the newly created **intro-to-hadoop** directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-04 11:37:40,299 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "-put: Not enough arguments: expected 1 but got 0\n",
      "Usage: hadoop fs [generic options]\n",
      "\t[-appendToFile <localsrc> ... <dst>]\n",
      "\t[-cat [-ignoreCrc] <src> ...]\n",
      "\t[-checksum <src> ...]\n",
      "\t[-chgrp [-R] GROUP PATH...]\n",
      "\t[-chmod [-R] <MODE[,MODE]... | OCTALMODE> PATH...]\n",
      "\t[-chown [-R] [OWNER][:[GROUP]] PATH...]\n",
      "\t[-copyFromLocal [-f] [-p] [-l] [-d] [-t <thread count>] <localsrc> ... <dst>]\n",
      "\t[-copyToLocal [-f] [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]\n",
      "\t[-count [-q] [-h] [-v] [-t [<storage type>]] [-u] [-x] [-e] <path> ...]\n",
      "\t[-cp [-f] [-p | -p[topax]] [-d] <src> ... <dst>]\n",
      "\t[-createSnapshot <snapshotDir> [<snapshotName>]]\n",
      "\t[-deleteSnapshot <snapshotDir> <snapshotName>]\n",
      "\t[-df [-h] [<path> ...]]\n",
      "\t[-du [-s] [-h] [-v] [-x] <path> ...]\n",
      "\t[-expunge [-immediate]]\n",
      "\t[-find <path> ... <expression> ...]\n",
      "\t[-get [-f] [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]\n",
      "\t[-getfacl [-R] <path>]\n",
      "\t[-getfattr [-R] {-n name | -d} [-e en] <path>]\n",
      "\t[-getmerge [-nl] [-skip-empty-file] <src> <localdst>]\n",
      "\t[-head <file>]\n",
      "\t[-help [cmd ...]]\n",
      "\t[-ls [-C] [-d] [-h] [-q] [-R] [-t] [-S] [-r] [-u] [-e] [<path> ...]]\n",
      "\t[-mkdir [-p] <path> ...]\n",
      "\t[-moveFromLocal <localsrc> ... <dst>]\n",
      "\t[-moveToLocal <src> <localdst>]\n",
      "\t[-mv <src> ... <dst>]\n",
      "\t[-put [-f] [-p] [-l] [-d] <localsrc> ... <dst>]\n",
      "\t[-renameSnapshot <snapshotDir> <oldName> <newName>]\n",
      "\t[-rm [-f] [-r|-R] [-skipTrash] [-safely] <src> ...]\n",
      "\t[-rmdir [--ignore-fail-on-non-empty] <dir> ...]\n",
      "\t[-setfacl [-R] [{-b|-k} {-m|-x <acl_spec>} <path>]|[--set <acl_spec> <path>]]\n",
      "\t[-setfattr {-n name [-v value] | -x name} <path>]\n",
      "\t[-setrep [-R] [-w] <rep> <path> ...]\n",
      "\t[-stat [format] <path> ...]\n",
      "\t[-tail [-f] [-s <sleep interval>] <file>]\n",
      "\t[-test -[defswrz] <path>]\n",
      "\t[-text [-ignoreCrc] <src> ...]\n",
      "\t[-touch [-a] [-m] [-t TIMESTAMP ] [-c] <path> ...]\n",
      "\t[-touchz <path> ...]\n",
      "\t[-truncate [-w] <length> <path> ...]\n",
      "\t[-usage [cmd ...]]\n",
      "\n",
      "Generic options supported are:\n",
      "-conf <configuration file>        specify an application configuration file\n",
      "-D <property=value>               define a value for a given property\n",
      "-fs <file:///|hdfs://namenode:port> specify default filesystem URL to use, overrides 'fs.defaultFS' property from configurations.\n",
      "-jt <local|resourcemanager:port>  specify a ResourceManager\n",
      "-files <file1,...>                specify a comma-separated list of files to be copied to the map reduce cluster\n",
      "-libjars <jar1,...>               specify a comma-separated list of jar files to be included in the classpath\n",
      "-archives <archive1,...>          specify a comma-separated list of archives to be unarchived on the compute machines\n",
      "\n",
      "The general command line syntax is:\n",
      "command [genericOptions] [commandOptions]\n",
      "\n",
      "Usage: hadoop fs [generic options] -put [-f] [-p] [-l] [-d] <localsrc> ... <dst>\n"
     ]
    }
   ],
   "source": [
    "!hdfs --config ~/hadoop_palmetto/config dfs -put"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-04 11:37:45,481 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2020-11-04 11:37:46,874 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n"
     ]
    }
   ],
   "source": [
    "!hdfs --config ~/hadoop_palmetto/config dfs -put text intro-to-hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-04 11:38:09,277 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 1 items\n",
      "drwxr-xr-x   - jin6 supergroup          0 2020-11-04 11:37 /user/jin6/intro-to-hadoop/text\n"
     ]
    }
   ],
   "source": [
    "!hdfs --config ~/hadoop_palmetto/config dfs -ls /user/jin6/intro-to-hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge \n",
    "\n",
    "Check the health status of the directories above in HDFS using fsck:\n",
    "```\n",
    "hdfs fsck <path-to-directory> -files -blocks -locations\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-04 11:38:17,421 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Connecting to namenode via http://node0297.palmetto.clemson.edu:9870/fsck?ugi=jin6&files=1&blocks=1&locations=1&path=%2Fuser%2Fjin6%2Fintro-to-hadoop%2Ftext%2Fcomplete-shakespeare.txt\n",
      "FSCK started by jin6 (auth:SIMPLE) from /10.125.2.108 for path /user/jin6/intro-to-hadoop/text/complete-shakespeare.txt at Wed Nov 04 11:38:18 EST 2020\n",
      "/user/jin6/intro-to-hadoop/text/complete-shakespeare.txt 5590193 bytes, replicated: replication=3, 1 block(s):  Under replicated BP-951723001-10.125.2.38-1604507141364:blk_1073741825_1001. Target Replicas is 3 but found 2 live replica(s), 0 decommissioned replica(s), 0 decommissioning replica(s).\n",
      "0. BP-951723001-10.125.2.38-1604507141364:blk_1073741825_1001 len=5590193 Live_repl=2  [DatanodeInfoWithStorage[10.125.2.102:9866,DS-57b0f0bb-cb1f-476d-b34b-8d8a852a2057,DISK], DatanodeInfoWithStorage[10.125.2.58:9866,DS-e010ccce-aa00-4980-ae64-e5b1ca439e7b,DISK]]\n",
      "\n",
      "\n",
      "Status: HEALTHY\n",
      " Number of data-nodes:\t2\n",
      " Number of racks:\t\t1\n",
      " Total dirs:\t\t\t0\n",
      " Total symlinks:\t\t0\n",
      "\n",
      "Replicated Blocks:\n",
      " Total size:\t5590193 B\n",
      " Total files:\t1\n",
      " Total blocks (validated):\t1 (avg. block size 5590193 B)\n",
      " Minimally replicated blocks:\t1 (100.0 %)\n",
      " Over-replicated blocks:\t0 (0.0 %)\n",
      " Under-replicated blocks:\t1 (100.0 %)\n",
      " Mis-replicated blocks:\t\t0 (0.0 %)\n",
      " Default replication factor:\t3\n",
      " Average block replication:\t2.0\n",
      " Missing blocks:\t\t0\n",
      " Corrupt blocks:\t\t0\n",
      " Missing replicas:\t\t1 (33.333332 %)\n",
      "\n",
      "Erasure Coded Block Groups:\n",
      " Total size:\t0 B\n",
      " Total files:\t0\n",
      " Total block groups (validated):\t0\n",
      " Minimally erasure-coded block groups:\t0\n",
      " Over-erasure-coded block groups:\t0\n",
      " Under-erasure-coded block groups:\t0\n",
      " Unsatisfactory placement block groups:\t0\n",
      " Average block group size:\t0.0\n",
      " Missing block groups:\t\t0\n",
      " Corrupt block groups:\t\t0\n",
      " Missing internal blocks:\t0\n",
      "FSCK ended at Wed Nov 04 11:38:18 EST 2020 in 4 milliseconds\n",
      "\n",
      "\n",
      "The filesystem under path '/user/jin6/intro-to-hadoop/text/complete-shakespeare.txt' is HEALTHY\n"
     ]
    }
   ],
   "source": [
    "!hdfs --config ~/hadoop_palmetto/config fsck intro-to-hadoop/text/complete-shakespeare.txt -files -blocks -locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MapReduce Programming Paradigm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is “map”?**\n",
    "– A function/procedure that is applied to every individual\n",
    "elements of a collection/list/array/…\n",
    "\n",
    "```\n",
    "int square(x) { return x*x;}\n",
    "map square [1,2,3,4] -> [1,4,9,16]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is “reduce”?**\n",
    "– A function/procedure that performs an operation on a list.\n",
    "This operation will “fold/reduce” this list into a single value\n",
    "(or a smaller subset)\n",
    "\n",
    "```\n",
    "reduce ([1,2,3,4]) using sum -> 10\n",
    "reduce ([1,2,3,4]) using multiply -> 24\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MapReduce is an old concept in functional programming. It is naturally applicable in HDFS: \n",
    "- `map` tasks are performed on top of individual data blocks (mainly to filter and decrease raw data contents while increase data value\n",
    "- `reduce` tasks are performed on intermediate results from `map` tasks (should now be significantly decreased in size) to calculate the final results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Hello World of Hadoop: Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘codes’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿The Project Gutenberg EBook of The Complete Works of William Shakespeare, by \n",
      "William Shakespeare\n",
      "\n",
      "This eBook is for the use of anyone anywhere at no cost and with\n",
      "almost no restrictions whatsoever.  You may copy it, give it away or\n",
      "re-use it under the terms of the Project Gutenberg License included\n",
      "with this eBook or online at www.gutenberg.org\n",
      "\n",
      "** This is a COPYRIGHTED Project Gutenberg eBook, Details Below **\n",
      "**     Please follow the copyright guidelines in this file.     **\n",
      "\n",
      "Title: The Complete Works of William Shakespeare\n",
      "\n",
      "Author: William Shakespeare\n",
      "\n",
      "Posting Date: September 1, 2011 [EBook #100]\n",
      "Release Date: January, 1994\n",
      "\n",
      "Language: English\n",
      "\n",
      "\n",
      "*** START OF THIS PROJECT GUTENBERG EBOOK COMPLETE WORKS--WILLIAM SHAKESPEARE ***\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Produced by World Library, Inc., from their Library of the Future\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "This is the 100th Etext file presented by Project Gutenberg, and\n",
      "is presented in cooperation with World Library, Inc., from their\n",
      "Library of the Future and Shakespeare CDROMS.  Project Gutenberg\n",
      "often releases Etexts that are NOT placed in the Public Domain!!\n",
      "\n",
      "Shakespeare\n",
      "\n",
      "*This Etext has certain copyright implications you should read!*\n",
      "\n",
      "<<THIS ELECTRONIC VERSION OF THE COMPLETE WORKS OF WILLIAM\n",
      "SHAKESPEARE IS COPYRIGHT 1990-1993 BY WORLD LIBRARY, INC., AND IS\n",
      "PROVIDED BY PROJECT GUTENBERG ETEXT OF ILLINOIS BENEDICTINE COLLEGE\n",
      "WITH PERMISSION.  ELECTRONIC AND MACHINE READABLE COPIES MAY BE\n",
      "DISTRIBUTED SO LONG AS SUCH COPIES (1) ARE FOR YOUR OR OTHERS\n",
      "PERSONAL USE ONLY, AND (2) ARE NOT DISTRIBUTED OR USED\n",
      "COMMERCIALLY.  PROHIBITED COMMERCIAL DISTRIBUTION INCLUDES BY ANY\n",
      "SERVICE THAT CHARGES FOR DOWNLOAD TIME OR FOR MEMBERSHIP.>>\n",
      "\n",
      "*Project Gutenberg is proud to cooperate with The World Library*\n",
      "in the presentation of The Complete Works of William Shakespeare\n",
      "for your reading for education and entertainment.  HOWEVER, THIS\n",
      "IS NEITHER SHAREWARE NOR PUBLIC DOMAIN. . .AND UNDER THE LIBRARY\n",
      "OF THE FUTURE CONDITIONS OF THIS PRESENTATION. . .NO CHARGES MAY\n",
      "BE MADE FOR *ANY* ACCESS TO THIS MATERIAL.  YOU ARE ENCOURAGED!!\n",
      "TO GIVE IT AWAY TO ANYONE YOU LIKE, BUT NO CHARGES ARE ALLOWED!!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "***** SMALL PRINT! for COMPLETE SHAKESPEARE *****\n",
      "\n",
      "THIS ELECTRONIC VERSION OF THE COMPLETE WORKS OF WILLIAM\n",
      "SHAKESPEARE IS COPYRIGHT 1990-1993 BY WORLD LIBRARY, INC.,\n",
      "AND IS PROVIDED BY PROJECT GUTENBERG ETEXT OF\n",
      "ILLINOIS BENEDICTINE COLLEGE WITH PERMISSION.\n",
      "\n",
      "Since unlike many other Project Gutenberg-tm etexts, this etext\n",
      "is copyright protected, and since the materials and methods you\n",
      "use will effect the Project's reputation, your right to copy and\n",
      "distribute it is limited by the copyright and other laws, and by\n",
      "the conditions of this \"Small Print!\" statement.\n",
      "\n",
      "1.  LICENSE\n",
      "\n",
      "  A) YOU MAY (AND ARE ENCOURAGED) TO DISTRIBUTE ELECTRONIC AND\n",
      "MACHINE READABLE COPIES OF THIS ETEXT, SO LONG AS SUCH COPIES\n",
      "(1) ARE FOR YOUR OR OTHERS PERSONAL USE ONLY, AND (2) ARE NOT\n",
      "DISTRIBUTED OR USED COMMERCIALLY.  PROHIBITED COMMERCIAL\n",
      "DISTRIBUTION INCLUDES BY ANY SERVICE THAT CHARGES FOR DOWNLOAD\n",
      "TIME OR FOR MEMBERSHIP.\n",
      "\n",
      "  B) This license is subject to the conditions that you honor\n",
      "the refund and replacement provisions of this \"small print!\"\n",
      "statement; and that you distribute exact copies of this etext,\n",
      "including this Small Print statement.  Such copies can be\n",
      "compressed or any proprietary form (including any form resulting\n",
      "from word processing or hypertext software), so long as\n",
      "*EITHER*:\n",
      "\n",
      "    (1) The etext, when displayed, is clearly readable, and does\n",
      "  *not* contain characters other than those intended by the\n",
      "  author of the work, although tilde (~), asterisk (*) and\n",
      "  underline (_) characters may be used to convey punctuation\n",
      "  intended by the author, and additional characters may be used\n",
      "  to indicate hypertext links; OR\n",
      "\n",
      "    (2) The etext is readily convertible by the reader at no\n",
      "  expense into plain ASCII, EBCDIC or equivalent form by the\n",
      "  program that displays the etext (as is the case, for instance,\n"
     ]
    }
   ],
   "source": [
    "!hdfs --config ~/hadoop_palmetto/config dfs -cat intro-to-hadoop/text/complete-shakespeare.txt \\\n",
    "    2>/dev/null | head -n 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting codes/wordcountMapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile codes/wordcountMapper.py\n",
    "#!/software/spackages/linux-centos8-x86_64/gcc-8.3.1/anaconda3-2019.10-v5cuhr6keyz5ryxcwvv2jkzfj2gwrj4a/bin/python                                          \n",
    "import sys                                                                                                \n",
    "for oneLine in sys.stdin:\n",
    "    oneLine = oneLine.strip()\n",
    "    for word in oneLine.split(\" \"):\n",
    "        if word != \"\":\n",
    "            print ('%s\\t%s' % (word, 1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿The\t1\n",
      "Project\t1\n",
      "Gutenberg\t1\n",
      "EBook\t1\n",
      "of\t1\n",
      "The\t1\n",
      "Complete\t1\n",
      "Works\t1\n",
      "of\t1\n",
      "William\t1\n",
      "Shakespeare,\t1\n",
      "by\t1\n",
      "William\t1\n",
      "Shakespeare\t1\n",
      "This\t1\n",
      "eBook\t1\n",
      "is\t1\n",
      "for\t1\n",
      "the\t1\n",
      "use\t1\n",
      "of\t1\n",
      "anyone\t1\n",
      "anywhere\t1\n",
      "at\t1\n",
      "no\t1\n",
      "cost\t1\n",
      "and\t1\n",
      "with\t1\n",
      "almost\t1\n",
      "no\t1\n",
      "restrictions\t1\n",
      "whatsoever.\t1\n",
      "You\t1\n",
      "may\t1\n",
      "copy\t1\n",
      "it,\t1\n",
      "give\t1\n",
      "it\t1\n",
      "away\t1\n",
      "or\t1\n",
      "re-use\t1\n",
      "it\t1\n",
      "under\t1\n",
      "the\t1\n",
      "terms\t1\n",
      "of\t1\n",
      "the\t1\n",
      "Project\t1\n",
      "Gutenberg\t1\n",
      "License\t1\n",
      "included\t1\n",
      "with\t1\n",
      "this\t1\n",
      "eBook\t1\n",
      "or\t1\n",
      "online\t1\n",
      "at\t1\n",
      "www.gutenberg.org\t1\n",
      "**\t1\n",
      "This\t1\n",
      "is\t1\n",
      "a\t1\n",
      "COPYRIGHTED\t1\n",
      "Project\t1\n",
      "Gutenberg\t1\n",
      "eBook,\t1\n",
      "Details\t1\n",
      "Below\t1\n",
      "**\t1\n",
      "**\t1\n",
      "Please\t1\n",
      "follow\t1\n",
      "the\t1\n",
      "copyright\t1\n",
      "guidelines\t1\n",
      "in\t1\n",
      "this\t1\n",
      "file.\t1\n",
      "**\t1\n",
      "Title:\t1\n",
      "The\t1\n",
      "Complete\t1\n",
      "Works\t1\n",
      "of\t1\n",
      "William\t1\n",
      "Shakespeare\t1\n",
      "Author:\t1\n",
      "William\t1\n",
      "Shakespeare\t1\n",
      "Posting\t1\n",
      "Date:\t1\n",
      "September\t1\n",
      "1,\t1\n",
      "2011\t1\n",
      "[EBook\t1\n",
      "#100]\t1\n",
      "Release\t1\n",
      "Date:\t1\n",
      "January,\t1\n",
      "1994\t1\n",
      "Language:\t1\n",
      "English\t1\n"
     ]
    }
   ],
   "source": [
    "!hdfs --config ~/hadoop_palmetto/config dfs -cat intro-to-hadoop/text/complete-shakespeare.txt \\\n",
    "    2>/dev/null \\\n",
    "    | head -n 20 \\\n",
    "    | python ./codes/wordcountMapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**\t1\n",
      "**\t1\n",
      "**\t1\n",
      "**\t1\n",
      "#100]\t1\n",
      "1,\t1\n",
      "1994\t1\n",
      "2011\t1\n",
      "a\t1\n",
      "almost\t1\n",
      "and\t1\n",
      "anyone\t1\n",
      "anywhere\t1\n",
      "at\t1\n",
      "at\t1\n",
      "Author:\t1\n",
      "away\t1\n",
      "Below\t1\n",
      "by\t1\n",
      "Complete\t1\n",
      "Complete\t1\n",
      "copy\t1\n",
      "copyright\t1\n",
      "COPYRIGHTED\t1\n",
      "cost\t1\n",
      "Date:\t1\n",
      "Date:\t1\n",
      "Details\t1\n",
      "eBook\t1\n",
      "eBook\t1\n",
      "eBook,\t1\n",
      "[EBook\t1\n",
      "EBook\t1\n",
      "English\t1\n",
      "file.\t1\n",
      "follow\t1\n",
      "for\t1\n",
      "give\t1\n",
      "guidelines\t1\n",
      "Gutenberg\t1\n",
      "Gutenberg\t1\n",
      "Gutenberg\t1\n",
      "in\t1\n",
      "included\t1\n",
      "is\t1\n",
      "is\t1\n",
      "it\t1\n",
      "it\t1\n",
      "it,\t1\n",
      "January,\t1\n",
      "Language:\t1\n",
      "License\t1\n",
      "may\t1\n",
      "no\t1\n",
      "no\t1\n",
      "of\t1\n",
      "of\t1\n",
      "of\t1\n",
      "of\t1\n",
      "of\t1\n",
      "online\t1\n",
      "or\t1\n",
      "or\t1\n",
      "Please\t1\n",
      "Posting\t1\n",
      "Project\t1\n",
      "Project\t1\n",
      "Project\t1\n",
      "Release\t1\n",
      "restrictions\t1\n",
      "re-use\t1\n",
      "September\t1\n",
      "Shakespeare\t1\n",
      "Shakespeare\t1\n",
      "Shakespeare\t1\n",
      "Shakespeare,\t1\n",
      "terms\t1\n",
      "the\t1\n",
      "the\t1\n",
      "the\t1\n",
      "the\t1\n",
      "﻿The\t1\n",
      "The\t1\n",
      "The\t1\n",
      "this\t1\n",
      "this\t1\n",
      "This\t1\n",
      "This\t1\n",
      "Title:\t1\n",
      "under\t1\n",
      "use\t1\n",
      "whatsoever.\t1\n",
      "William\t1\n",
      "William\t1\n",
      "William\t1\n",
      "William\t1\n",
      "with\t1\n",
      "with\t1\n",
      "Works\t1\n",
      "Works\t1\n",
      "www.gutenberg.org\t1\n",
      "You\t1\n"
     ]
    }
   ],
   "source": [
    "!hdfs --config ~/hadoop_palmetto/config dfs -cat intro-to-hadoop/text/complete-shakespeare.txt \\\n",
    "    2>/dev/null \\\n",
    "    | head -n 20 \\\n",
    "    | python ./codes/wordcountMapper.py \\\n",
    "    | sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting codes/wordcountReducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile codes/wordcountReducer.py\n",
    "#!/software/spackages/linux-centos8-x86_64/gcc-8.3.1/anaconda3-2019.10-v5cuhr6keyz5ryxcwvv2jkzfj2gwrj4a/bin/python\n",
    "import sys\n",
    "\n",
    "current_word = None\n",
    "total_word_count = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    word, count = line.split(\"\\t\", 1)\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        continue\n",
    "    \n",
    "    if current_word == word:\n",
    "        total_word_count += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            print (\"%s\\t%s\" % (current_word, total_word_count))\n",
    "        current_word = word\n",
    "        total_word_count = 1\n",
    "        \n",
    "if current_word == word:\n",
    "    print (\"%s\\t%s\" % (current_word, total_word_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**\t4\n",
      "#100]\t1\n",
      "1,\t1\n",
      "1994\t1\n",
      "2011\t1\n",
      "a\t1\n",
      "almost\t1\n",
      "and\t1\n",
      "anyone\t1\n",
      "anywhere\t1\n",
      "at\t2\n",
      "Author:\t1\n",
      "away\t1\n",
      "Below\t1\n",
      "by\t1\n",
      "Complete\t2\n",
      "copy\t1\n",
      "copyright\t1\n",
      "COPYRIGHTED\t1\n",
      "cost\t1\n",
      "Date:\t2\n",
      "Details\t1\n",
      "eBook\t2\n",
      "eBook,\t1\n",
      "[EBook\t1\n",
      "EBook\t1\n",
      "English\t1\n",
      "file.\t1\n",
      "follow\t1\n",
      "for\t1\n",
      "give\t1\n",
      "guidelines\t1\n",
      "Gutenberg\t3\n",
      "in\t1\n",
      "included\t1\n",
      "is\t2\n",
      "it\t2\n",
      "it,\t1\n",
      "January,\t1\n",
      "Language:\t1\n",
      "License\t1\n",
      "may\t1\n",
      "no\t2\n",
      "of\t5\n",
      "online\t1\n",
      "or\t2\n",
      "Please\t1\n",
      "Posting\t1\n",
      "Project\t3\n",
      "Release\t1\n",
      "restrictions\t1\n",
      "re-use\t1\n",
      "September\t1\n",
      "Shakespeare\t3\n",
      "Shakespeare,\t1\n",
      "terms\t1\n",
      "the\t4\n",
      "﻿The\t1\n",
      "The\t2\n",
      "this\t2\n",
      "This\t2\n",
      "Title:\t1\n",
      "under\t1\n",
      "use\t1\n",
      "whatsoever.\t1\n",
      "William\t4\n",
      "with\t2\n",
      "Works\t2\n",
      "www.gutenberg.org\t1\n",
      "You\t1\n"
     ]
    }
   ],
   "source": [
    "!hdfs --config ~/hadoop_palmetto/config dfs -cat intro-to-hadoop/text/complete-shakespeare.txt \\\n",
    "    2>/dev/null \\\n",
    "    | head -n 20 \\\n",
    "    | python ./codes/wordcountMapper.py \\\n",
    "    | sort \\\n",
    "    | python ./codes/wordcountReducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-04 11:39:13,134 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "rm: `intro-to-hadoop/output-wordcount': No such file or directory\n",
      "2020-11-04 11:39:15,904 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "2020-11-04 11:39:16,068 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [./codes/wordcountMapper.py, ./codes/wordcountReducer.py] [/software/spackages/linux-centos8-x86_64/gcc-8.3.1/hadoop-3.2.1-lux74iyfcxxzatxh4em67xezesi4z4i4/share/hadoop/tools/lib/hadoop-streaming-3.2.1.jar] /local_scratch/pbs.828445.pbs02/streamjob416015679607181280.jar tmpDir=null\n",
      "2020-11-04 11:39:17,186 INFO client.RMProxy: Connecting to ResourceManager at node0297.palmetto.clemson.edu/10.125.2.38:8050\n",
      "2020-11-04 11:39:17,461 INFO client.RMProxy: Connecting to ResourceManager at node0297.palmetto.clemson.edu/10.125.2.38:8050\n",
      "2020-11-04 11:39:17,732 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/jin6/.staging/job_1604507172335_0001\n",
      "2020-11-04 11:39:17,859 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-11-04 11:39:17,972 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-11-04 11:39:18,007 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-11-04 11:39:18,084 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2020-11-04 11:39:18,114 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-11-04 11:39:18,137 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-11-04 11:39:18,151 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "2020-11-04 11:39:18,398 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-11-04 11:39:18,458 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1604507172335_0001\n",
      "2020-11-04 11:39:18,459 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2020-11-04 11:39:18,687 INFO conf.Configuration: resource-types.xml not found\n",
      "2020-11-04 11:39:18,687 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2020-11-04 11:39:19,132 INFO impl.YarnClientImpl: Submitted application application_1604507172335_0001\n",
      "2020-11-04 11:39:19,172 INFO mapreduce.Job: The url to track the job: http://node0297.palmetto.clemson.edu:8088/proxy/application_1604507172335_0001/\n",
      "2020-11-04 11:39:19,176 INFO mapreduce.Job: Running job: job_1604507172335_0001\n",
      "2020-11-04 11:39:28,330 INFO mapreduce.Job: Job job_1604507172335_0001 running in uber mode : false\n",
      "2020-11-04 11:39:28,331 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2020-11-04 11:39:37,415 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2020-11-04 11:39:46,478 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2020-11-04 11:39:46,489 INFO mapreduce.Job: Job job_1604507172335_0001 completed successfully\n",
      "2020-11-04 11:39:46,644 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=8575076\n",
      "\t\tFILE: Number of bytes written=17850189\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=5678979\n",
      "\t\tHDFS: Number of bytes written=721220\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=425152\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=97184\n",
      "\t\tTotal time spent by all map tasks (ms)=13286\n",
      "\t\tTotal time spent by all reduce tasks (ms)=6074\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=13286\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=6074\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=54419456\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=12439552\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=124796\n",
      "\t\tMap output records=904087\n",
      "\t\tMap output bytes=6766896\n",
      "\t\tMap output materialized bytes=8575082\n",
      "\t\tInput split bytes=298\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=67799\n",
      "\t\tReduce shuffle bytes=8575082\n",
      "\t\tReduce input records=904087\n",
      "\t\tReduce output records=67799\n",
      "\t\tSpilled Records=1808174\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=396\n",
      "\t\tCPU time spent (ms)=11880\n",
      "\t\tPhysical memory (bytes) snapshot=592543744\n",
      "\t\tVirtual memory (bytes) snapshot=5609619456\n",
      "\t\tTotal committed heap usage (bytes)=343277568\n",
      "\t\tPeak Map Physical memory (bytes)=272580608\n",
      "\t\tPeak Map Virtual memory (bytes)=1916157952\n",
      "\t\tPeak Reduce Physical memory (bytes)=111292416\n",
      "\t\tPeak Reduce Virtual memory (bytes)=1860923392\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=5678681\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=721220\n",
      "2020-11-04 11:39:46,644 INFO streaming.StreamJob: Output directory: intro-to-hadoop/output-wordcount\n"
     ]
    }
   ],
   "source": [
    "!hdfs --config ~/hadoop_palmetto/config dfs -rm -R intro-to-hadoop/output-wordcount\n",
    "!mapred --config ~/hadoop_palmetto/config streaming \\\n",
    "    -input intro-to-hadoop/text/complete-shakespeare.txt \\\n",
    "    -output intro-to-hadoop/output-wordcount \\\n",
    "    -file ./codes/wordcountMapper.py \\\n",
    "    -mapper wordcountMapper.py \\\n",
    "    -file ./codes/wordcountReducer.py \\\n",
    "    -reducer wordcountReducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-04 11:39:57,801 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 2 items\n",
      "-rw-r--r--   3 jin6 supergroup          0 2020-11-04 11:39 intro-to-hadoop/output-wordcount/_SUCCESS\n",
      "-rw-r--r--   3 jin6 supergroup     721220 2020-11-04 11:39 intro-to-hadoop/output-wordcount/part-00000\n"
     ]
    }
   ],
   "source": [
    "!hdfs --config ~/hadoop_palmetto/config dfs -ls intro-to-hadoop/output-wordcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\t241\n",
      "\"'Tis\t1\n",
      "\"A\t4\n",
      "\"AS-IS\".\t1\n",
      "\"Air,\"\t1\n",
      "\"Alas,\t1\n",
      "\"Amen\"\t2\n",
      "\"Amen\"?\t1\n",
      "\"Amen,\"\t1\n",
      "\"And\t1\n",
      "\"Aroint\t1\n",
      "\"B\t1\n",
      "\"Black\t1\n",
      "\"Break\t1\n",
      "\"Brutus\"\t1\n",
      "\"Brutus,\t2\n",
      "\"C\t1\n",
      "\"Caesar\"?\t1\n",
      "\"Caesar,\t1\n",
      "\"Caesar.\"\t2\n",
      "\"Certes,\"\t1\n",
      "\"Come\t1\n",
      "\"Cursed\t1\n",
      "\"D\t1\n",
      "\"Darest\t1\n",
      "\"Defect\"\t1\n",
      "\"Defects,\"\t1\n",
      "\"Do\t1\n",
      "\"E\t1\n",
      "\"Fear\t2\n",
      "\"Fly,\t1\n",
      "\"Gentle\t1\n",
      "\"Give\t2\n",
      "\"Glamis\t1\n",
      "\"God\t2\n",
      "\"Good\t1\n",
      "\"Havoc!\"\t1\n",
      "\"He\t1\n",
      "\"Help\t1\n",
      "\"Help,\t2\n",
      "\"Here\t1\n",
      "\"Hold,\t2\n",
      "\"I\t4\n",
      "\"Indeed!\"\t1\n",
      "\"Information\t1\n",
      "\"King\t1\n",
      "\"Liberty,\t1\n",
      "\"Lo,\t1\n",
      "\"Long\t1\n",
      "\"Murther!\"\t2\n",
      "\"Neither\t1\n",
      "\"Now\t1\n",
      "\"O\t2\n",
      "\"Peace,\t1\n",
      "\"Plain\t2\n",
      "\"Pro-\t1\n",
      "\"Project\t5\n",
      "\"Right\t2\n",
      "\"Shall\t1\n",
      "\"Sing\t2\n",
      "\"Sir,\t1\n",
      "\"Sleep\t2\n",
      "\"Small\t2\n",
      "\"Speak,\t1\n",
      "\"Sweet\t1\n",
      "\"That\t1\n",
      "\"The\t1\n",
      "\"These\t1\n",
      "\"They\t2\n",
      "\"This\t2\n",
      "\"Thus\t2\n",
      "\"Tis\t2\n",
      "\"Where\t1\n",
      "\"Willow,\t1\n",
      "\"You'll\t1\n",
      "\"better\"?\t1\n",
      "\"hem,\"\t1\n",
      "\"never.\"\t1\n",
      "\"not\"\t1\n",
      "\"small\t1\n",
      "\"then\"\t1\n",
      "\"thrusting\"\t1\n",
      "\"thy\t1\n",
      "\"twas\t1\n",
      "\"whore\"\t1\n",
      "\"whore.\"\t1\n",
      "\"willow\";\t1\n",
      "#10000,\t2\n",
      "#100]\t1\n",
      "$5,000)\t1\n",
      "&\t3\n",
      "&C.\t2\n",
      "&c.\t12\n",
      "&c.'\t2\n",
      "&c.,\t2\n",
      "'\"All\t1\n",
      "'\"Among\t1\n",
      "'\"And,\t1\n",
      "'\"But,\t1\n",
      "'\"Gamut\"\t1\n"
     ]
    }
   ],
   "source": [
    "!hdfs --config ~/hadoop_palmetto/config dfs -cat intro-to-hadoop/output-wordcount/part-00000 \\\n",
    "    2>/dev/null | head -n 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "Modify *wordcountMapper.py* so that punctuations and capitalization are no longer factors in determining unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%writefile codes/wordcountEnhancedMapper.py\n",
    "#!/software/spackages/linux-centos8-x86_64/gcc-8.3.1/anaconda3-2019.10-v5cuhr6keyz5ryxcwvv2jkzfj2gwrj4a/bin/python                                          \n",
    "import sys                     \n",
    "import string\n",
    "\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "for oneLine in sys.stdin:\n",
    "    oneLine = oneLine.strip()\n",
    "    for word in oneLine.split(\" \"):\n",
    "        if word != \"\":\n",
    "            newWord = word.translate(translator).lower()\n",
    "            print ('%s\\t%s' % (_______, 1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -rm -R intro-to-hadoop/output-wordcount-enhanced\n",
    "!ssh dsciutil yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \\\n",
    "    -input intro-to-hadoop/text/gutenberg-shakespeare.txt \\\n",
    "    -output intro-to-hadoop/output-wordcount \\\n",
    "    -file ____________________________________________________ \\\n",
    "    -mapper _____________________ \\\n",
    "    -file ____________________________________________________ \\\n",
    "    -reducer _____________________ \\"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Anaconda 2019.10)",
   "language": "python",
   "name": "anaconda3-2019.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
